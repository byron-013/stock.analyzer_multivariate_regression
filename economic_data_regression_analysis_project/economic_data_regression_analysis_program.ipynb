{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING FILEPATH AND IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan, het_goldfeldquandt, het_white\n",
    "from scipy import stats\n",
    "\n",
    "### IMPORT FILES FROM DATA_FUNCTIONS - PROVIDED IN PROJET FOLDER\n",
    "### MAKE SURE TO PUT YOUR FILE PATH HERE\n",
    "### FOR EXAMPLE - r'C:\\Users\\rest of your path to the folder labled data functions'\n",
    "sys.path.insert(0, r'YOUR FILEPATH TO THE DATA_FUNCTIONS FOLDER')\n",
    "import data_combine, data_financial_import, data_financial_CSVimport, data_indicator_bollingerbands, data_indicator_movingaverage, data_indicator_pricechange, data_indicator_tradingvolume, data_indicator_volatility, data_outliers_by_column, data_standardize_timeseries, data_transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE DATE RANGE AND TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'NVDA' \n",
    "start_date = '2019-01-01'\n",
    "end_date = '2022-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET DATA FROM USER PROVIDED FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST ONLY USER PROVIDED CSV FILES WITH ECONOMIC DATA - LEAVE ECONOMIC_DATA.CSV IN THE LIST IF YOU ARE USING DATA FROM IT.\n",
    "### ANY FILES IN THE LIST MUST BE INDEXED BY DATE\n",
    "\n",
    "csv_files_userProvided = ['economic_data.csv']\n",
    "\n",
    "for csv_file in csv_files_userProvided:\n",
    "    data_financial_CSVimport.get_economic_data(csv_file, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET YFINANCE DATA AND CALCULATE INDICATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_financial_import.download_stock_data(ticker, start_date, end_date)\n",
    "data_indicator_bollingerbands.calculate_bollinger_bands(ticker, start_date, end_date)\n",
    "data_indicator_movingaverage.calculate_moving_averages(ticker, start_date, end_date)\n",
    "data_indicator_pricechange.calculate_price_change(ticker, start_date, end_date)\n",
    "data_indicator_tradingvolume.calculate_trading_volume(ticker, start_date, end_date)\n",
    "data_indicator_volatility.calculate_volatility(ticker, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STANDARDIZE DATA - HERE YOU CHOOSE THE FREQUENCY FOR ALL OF THE GATHERED DATA\n",
    "\n",
    "--- ALL CSV FILES SHOULD BE OVER THE SAME DATE RANGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY ADD USER PROVEDED CSV FILES TO THIS LIST\n",
    "### ECONOMIC_DATA.CSV AND INDICATOR/STOCK DATA ARE ALREADY INCLUDED \n",
    "\n",
    "csv_files = [f'{ticker}_data.csv', 'selected_economic_data.csv', f'indicator_{ticker}_bollinger_bands.csv', f'indicator_{ticker}_moving_averages.csv', f'indicator_{ticker}_price_change.csv', f'indicator_{ticker}_trading_volume.csv', f'indicator_{ticker}_volatility.csv']\n",
    "\n",
    "### DEFINE FREQUENCY AND FILL METHOD\n",
    "frequency = 'weekly'  # Replace with your desired frequency\n",
    "fill_method = 'fill_avg'  # Replace with your desired fill method\n",
    "\n",
    "new_files = []\n",
    "for csv_file in csv_files:\n",
    "    new_file = data_standardize_timeseries.standardize_data(csv_file, frequency, fill_method)\n",
    "    new_files.append(new_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINE THE STANDARDIZED CSVS TO HAVE ALL DATA IN ONE CSV ORGANIZED BY COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combine.merge_csv_files(new_files, 'variable_data_menu.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VARIABLE SELECTION\n",
    "\n",
    "---HERE IS WHERE YOU SHOULD COME BACK TO MAKE CHANGES TO YOUR VARIABLE CHOICES IF NEEDED.              \n",
    "---MAKE SURE TO RUN ALL MODULES FOLLOWING THIS ONE AFTER MAKING VARIABLE CHANHGES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('variable_data_menu.csv')\n",
    "\n",
    "dependent_variable = 'Close'  # Replace with your actual column title\n",
    "independent_variables = ['Daily Change','Unemployment Rate', 'rf_13 WEEK']  # Replace with your actual column titles\n",
    "\n",
    "index_column = 'Date'  # CHOOSE A COLUMN TO INDEX YOUR DATA\n",
    "\n",
    "\n",
    "\n",
    "y = df[[dependent_variable]]\n",
    "X = df[independent_variables]\n",
    "index_data = df[index_column]\n",
    "y.set_index(index_data, inplace=True)\n",
    "X.set_index(index_data, inplace=True)\n",
    "y.to_csv('dependent_variable_data.csv')\n",
    "X.to_csv('independent_variable_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLIT INDEPENDENT AND DEPENDENT VARIABLE DATA INTO TRAIN AND TEST DATA SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable_data = pd.read_csv('dependent_variable_data.csv', index_col=0)\n",
    "independent_variable_data = pd.read_csv('independent_variable_data.csv', index_col=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(independent_variable_data, dependent_variable_data, test_size=0.2, random_state=42)\n",
    "\n",
    "### SAVE THE TRAINING AND TEST SETS TO NEW CSV FILES\n",
    "X_train.to_csv('train_independent_variable_data.csv')\n",
    "X_test.to_csv('test_independent_variable_data.csv')\n",
    "y_train.to_csv('train_dependent_variable_data.csv')\n",
    "y_test.to_csv('test_dependent_variable_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGRESSION DIAGNOSTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train_independent_variable_data.csv', index_col=0)\n",
    "y_train = pd.read_csv('train_dependent_variable_data.csv', index_col=0)\n",
    "\n",
    "### FIT THE MODEL\n",
    "model = sm.OLS(y_train, sm.add_constant(X_train))\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "print()\n",
    "print()\n",
    "                                            ### HOW TO INTERPRET THE MODEL SUMMARY:\n",
    "\n",
    "### R-squared: This is the coefficient of determination. It tells you the proportion of the variance in the dependent variable that can be explained by the independent variables.\n",
    "### R-squared values range from 0 to 1. An R-squared of 100% indicates that all changes in the dependent variable are completely explained by changes in the independent variables.\n",
    "\n",
    "### Adjusted R-squared: This is the R-squared adjusted based on the number of predictors in the model. \n",
    "### It only increases if the new predictor improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.\n",
    "\n",
    "### F-statistic: This is a good indicator of whether there is a relationship between the predictor and the response variable. The further the F-statistic is from 1, the better it is.\n",
    "\n",
    "### coef: These are the coefficients of the independent variables in the regression equation.\n",
    "\n",
    "### std err: These are the standard errors of the coefficients. The standard errors can be used to compute confidence intervals and to carry out hypothesis tests for the coefficients.\n",
    "\n",
    "### P > |t|: These are the p-values associated with the null hypothesis that the coefficient equals zero (no effect). \n",
    "### A low p-value (< 0.05) indicates that you can reject the null hypothesis.\n",
    "\n",
    "### Confidence Interval: These are the 95% confidence intervals for the coefficients. If zero is not in the interval, it suggests the coefficient is significantly different from zero.\n",
    "\n",
    "\n",
    "\n",
    "###                                               CHECKING OLS ASSUMPTIONS\n",
    "print('CHECKING OLS ASSUMPTIONS:')\n",
    "print()\n",
    "print()\n",
    "### Linearity: A scatter plot of observed vs. predicted values is used to check the linearity assumption. \n",
    "###             If the relationship is linear, the points should roughly form a straight line.\n",
    "plt.scatter(y_train, results.predict())\n",
    "plt.xlabel('Observed')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Check for Linearity')\n",
    "print('Linearity: A scatter plot of observed vs. predicted values is used to check the linearity assumption.')\n",
    "print('If the relationship is linear, the points should roughly form a straight line.')\n",
    "plt.show()\n",
    "print()\n",
    "### Independence: The Durbin-Watson test is used to check the independence assumption. \n",
    "###                 The test statistic is approximately equal to 2*(1-r), where r is the sample autocorrelation of the residuals. \n",
    "###                 Thus, for r == 0, indicating no serial correlation, the test statistic equals 2.\n",
    "print('Independence: The Durbin-Watson test is used to check the independence assumption.')\n",
    "print('The test statistic is approximately equal to 2*(1-r), where r is the sample autocorrelation of the residuals.')\n",
    "print('Thus, for r == 0, indicating no serial correlation, the test statistic equals 2.')\n",
    "print('Durbin-Watson test statistic:', sm.stats.durbin_watson(results.resid))\n",
    "print()\n",
    "### Homoscedasticity:A scatter plot of residuals vs. predicted values is used to check the homoscedasticity assumption. \n",
    "###                  If the variance of the errors is constant across all levels of the independent variables, the plot should show a cloud of points without a clear pattern.\n",
    "plt.scatter(results.predict(), results.resid)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Check for Homoscedasticity')\n",
    "print('Homoscedasticity: A scatter plot of residuals vs. predicted values is used to check the homoscedasticity assumption.')\n",
    "print('If the variance of the errors is constant across all levels of the independent variables, the plot should show a cloud of points without a clear pattern.')\n",
    "plt.show()\n",
    "print()\n",
    "### Multicollinearity: The Variance Inflation Factor (VIF) is used to check the multicollinearity assumption. \n",
    "###                     VIF quantifies how much the variance is inflated due to multicollinearity. \n",
    "###                     VIF of 1 indicates no correlation, while VIF exceeding 5 or 10 indicates high multicollinearity.\n",
    "vif = pd.DataFrame()\n",
    "vif['VIF Factor'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "vif['features'] = X_train.columns\n",
    "print('Variance Inflation Factor (VIF) is used to check the multicollinearity assumption.')\n",
    "print('VIF of 1 indicates no correlation, while VIF exceeding 5 or 10 indicates high multicollinearity.')\n",
    "print()\n",
    "print(vif)\n",
    "print()\n",
    "### Normality: A QQ plot of the residuals and the Shapiro-Wilk test are used to check the normality assumption. \n",
    "###            If the errors are normally distributed, the points in the QQ plot should roughly fall on a straight line. \n",
    "###            The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.\n",
    "sm.qqplot(results.resid, line='s')\n",
    "plt.title('Check for Normality')\n",
    "print('Normality: A QQ plot of the residuals and the Shapiro-Wilk test are used to check the normality assumption.')\n",
    "print('If the errors are normally distributed, the points in the QQ plot should roughly fall on a straight line.')\n",
    "plt.show()\n",
    "print()\n",
    "print('The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution.')\n",
    "print('Shapiro-Wilk test:', stats.shapiro(results.resid))\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "###                                           CHECKING FOR NORMALITY IN ERROR TERMS\n",
    "print('CHECKING FOR NORMALITY IN ERROR TERMS!')\n",
    "print()\n",
    "print()\n",
    "### Histogram of Residuals: This plot shows the distribution of residuals. It’s used to check the assumption of normality in the error terms. \n",
    "### Ideally, the residuals should be normally distributed, which means the histogram should resemble a bell curve. If the residuals are not normally distributed, it \n",
    "### may suggest that linear regression is not the best model for the data.\n",
    "plt.hist(results.resid, bins=30)\n",
    "plt.xlabel('Residual')\n",
    "plt.title('Histogram of Residuals')\n",
    "print('Histogram of Residuals: This plot shows the distribution of residuals. It is used to check the assumption of normality in the error terms.')\n",
    "print('If the data is not bell curve shaped then then the assumption of normally distributed errors was false.')\n",
    "print()\n",
    "plt.show()\n",
    "print()\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "###                                             CHECKING FOR HETEROSCEDASTICITY\n",
    "### The three statistical tests used to check for heteroskedasticity are the Breusch-Pagan test, Goldfeld-Quandt test, and White test. These tests have null \n",
    "### hypothesis that the error variances are all equal (homoscedasticity) and alternative hypothesis that the error variances are not equal (heteroskedasticity).\n",
    "print('CHECKING FOR HETEROSKEDASTICITY:')\n",
    "print()\n",
    "print()\n",
    "bp_test = het_breuschpagan(results.resid, results.model.exog)\n",
    "print('The Breusch-Pagan test is a statistical procedure that returns four key values. The first is the Lagrange multiplier statistic, which serves as the test statistic.') \n",
    "print('The second is the p-value of the Lagrange multiplier test. If this value is less than your chosen significance level, it indicates') \n",
    "print('that you should reject the null hypothesis of homoskedasticity. The third value is the f-value of the hypothesis that the error variance does not depend on x, providing') \n",
    "print('another test statistic. The fourth and final value is the p-value of the f statistic. Again, if this value is less than your chosen significance level,') \n",
    "print('it suggests that the null hypothesis of homoskedasticity should be rejected.')\n",
    "print()\n",
    "print(f'Breusch-Pagan test: {bp_test}')\n",
    "print()\n",
    "print()\n",
    "\n",
    "gq_test = het_goldfeldquandt(y_train, X_train)\n",
    "print('The Goldfeld-Quandt test is another statistical test that returns three values. The first is the value of the test statistic. If this value is significantly different')\n",
    "print('from zero, it indicates the presence of heteroskedasticity. The second value is the p-value for a two-sided test for the null hypothesis that the variance of the ')\n",
    "print('residuals is the same across the range of data. If this value is less than your chosen significance level, it suggests that the null hypothesis of homoskedasticity should be rejected.') \n",
    "print('The third value indicates whether the test statistic is positive increasing or negative decreasing, which can provide insight into the nature of the heteroskedasticity.')\n",
    "print()\n",
    "print(f'Goldfeld-Quandt test: {gq_test}')\n",
    "print()\n",
    "print()\n",
    "\n",
    "white_test = het_white(results.resid, results.model.exog)\n",
    "print('The White test is a statistical procedure that also returns four values. The first is the White test statistic. The second is the p-value for the test statistic.')\n",
    "print('If this value is less than your chosen significance level, it indicates that you should reject the null hypothesis of homoskedasticity. The third value is ')\n",
    "print('the F-statistic of the hypothesis that the error variance does not depend on x, providing another test statistic. The fourth and final value is the p-value for the')\n",
    "print('F-statistic. Again, if this value is less than your chosen significance level, it suggests that the null hypothesis of homoskedasticity should be rejected.')\n",
    "print()\n",
    "print(f'White test: {white_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- IF YOU NEED TO MAKE CHANGES TO VARIABLE CHOICE REMEMBER THAT IT NEEDS TO BE DONE IN THE VARIABLE SELECTION MODULE ABOVE. \n",
    "\n",
    "--- MAKE SURE TO RUN THAT MODULE, THE MODULE TO SPLIT THE DATA, AND RUN THE DIAGNOSTIC MODULE AGAIN SO THAT THE REGRESSION GETS RUN AGAIN WITH THE CHANGES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE ERROR TYPE AND GET REGRESSION RESULTS\n",
    "\n",
    "--- AFTER MAKING ALL VARIABLE CHANGES YOU CAN SEE WHAT A CHANGE IN ERROR TYPE WILL DO TO THE MODEL. [IF NEEDED/INDICATED BY THE HETEROSKEDASTICITY TESTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_type = 'HC0'      # 'HC0', 'HC1', 'HC2', 'HC3' or None for OLS\n",
    "\n",
    "### HC0: This is the basic White estimator and can be used when there is heteroscedasticity.\n",
    "### HC1: This adjusts the HC0 estimator for degrees of freedom and is useful when the sample size is small.\n",
    "### HC2: This adjusts for leverage, which can be helpful when there are outliers in the predictor variables.\n",
    "### HC3: This further adjusts for extreme leverage points and can be beneficial when there are extreme outliers in the predictor variables.\n",
    "\n",
    "if error_type in ['HC0', 'HC1', 'HC2', 'HC3']:\n",
    "    robust_results = results.get_robustcov_results(cov_type=error_type)\n",
    "    print('ROBUST ERROR OLS REGRESSION RESULTS:')\n",
    "    print(robust_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHEN DONE WORKING WITH THE TRAIN DATA RUN THE REGRESSION ON THE TEST DATA SET AND MAKE SURE THE RESULTS ARE AS EXPECTED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test_independent_variable_data.csv', index_col=0)\n",
    "y_test = pd.read_csv('test_dependent_variable_data.csv', index_col=0)\n",
    "\n",
    "# FIT MODEL ON TEST DATA\n",
    "model = sm.OLS(y_test, sm.add_constant(X_test))\n",
    "results = model.fit()\n",
    "print(results.summary())\n",
    "print()\n",
    "\n",
    "#DEFINE ERROR TYPE \n",
    "#MAKE SURE IT IS THE SAME ONE YOU DECIDED WAS BEST FOR TRAINING DATA\n",
    "testdata_error_type = 'HC0'  #'HC0', 'HC1', 'HC2', 'HC3' or None for OLS\n",
    "\n",
    "print()\n",
    "#GET TEST DATA REGRESSION RESULTS (WITH ROBUST ERROR RESULTS AS WELL WHEN APPLICABLE)\n",
    "if testdata_error_type in ['HC0', 'HC1', 'HC2', 'HC3']:\n",
    "    robust_results = results.get_robustcov_results(cov_type=testdata_error_type)\n",
    "    print('ROBUST ERROR OLS REGRESSION RESULTS:')\n",
    "    print(robust_results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF THERE ARE STILL ISSUES RUNNING THE REGRESSION BECAUSE OF THE DATA THEN CHECK OUT THE .PY FILE WITH DATA TRANSFORMATIONS THAT SOMETIMES CAN MAKE A VARIABLE USEABLE IN A LINEAR REGRESSION. \n",
    "\n",
    "YOU CAN TRANSFORM THE DATA WITH THE FUNCTIONS IN THE DATA_TRANSFORMATIONS.PY FILE. APPLY TRANSFORMATION TO COLUMNS THEN TO USE IT IN THE REGRESSION INCORPORATE IT IN THE USER PROVIDED FILES AND RUN THE WHOLE PROGRAM AGAIN.\n",
    "\n",
    "DIRECTION REGARDING WHEN EACH TRANSFORMATION SHOULD BE USED IS IN THE .PY FILE UNDER THE FUNCTIONS.\n",
    "\n",
    "ALSO MORE REGRESSION MODELS WILL BE COMING SOON TO MY GITHUB FOR CASES WHERE OLS CONDITIONS CAUSE RESULTS TO BE INACCURATE. \n",
    "WILL HAVE POLY REGRESSION, POISSON REGRESSION, AND PANEL DATA REGRESSION WITH FIXED AND RANDOM EFFECTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
